# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ofAdNaITYCC0_QBbDvrmhaauu4mpBkYh
"""

#to install packages for t5 model and to handle dataset
!pip install transformers datasets -q

import json
import random
from datasets import Dataset
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments

#To load the dataset
with open("/content/data.json", "r") as f:
    raw_data = json.load(f)

#We are converting into supervised form(to map single output for each platform)
def convert_to_supervised(entry, platform):
    return {
        "input": f"repurpose for {platform}: {entry['input_transcript']}",
        "target": entry[platform]
        if platform != "tweet_thread"
        else " <sep> ".join(entry["tweet_thread"])
    }

#For calling for each platform
examples = []
for entry in raw_data:
    for platform in ["linkedin_post", "tweet_thread", "youtube_description", "instagram_caption"]:
        examples.append(convert_to_supervised(entry, platform))

#shuffling the dataset so that we can avoid bias
random.shuffle(examples)
dataset = Dataset.from_list(examples)

#To load T5 tokenizer and model
tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

#Tokenization (convert text to tokens(numbers))
def preprocess(batch):
    input_enc = tokenizer(batch["input"], padding="max_length", truncation=True, max_length=512)
    target_enc = tokenizer(batch["target"], padding="max_length", truncation=True, max_length=128)

    return {
        "input_ids": input_enc["input_ids"],
        "attention_mask": input_enc["attention_mask"],
        "labels": target_enc["input_ids"]
    }

dataset = dataset.map(preprocess)

#Training Argument
from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="genai-model",
    per_device_train_batch_size=2,
    num_train_epochs=5,
    logging_steps=10,
    save_strategy="no",
    remove_unused_columns=False # Added this line
)

#Trainer Setup
trainer= Trainer(
    model=model,
    args=args,
    train_dataset=dataset
)

#Training the model
import os
os.environ["WANDB_DISABLED"] = "true"
trainer.train()

#Saving the model
model.save_pretrained("genai-model")
tokenizer.save_pretrained("genai-model")

from transformers import T5Tokenizer, T5ForConditionalGeneration

model = T5ForConditionalGeneration.from_pretrained("genai-model")
tokenizer = T5Tokenizer.from_pretrained("genai-model")

def generate_output(transcript, platform):
    prompt = f"repurpose for {platform}: {transcript}"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True)
    output_ids = model.generate(
        inputs["input_ids"],
        max_length=150,
        num_beams=5,
        early_stopping=True
    )
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

test_transcript = "Staying hydrated helps boost brain function and energy levels."
platform = "linkedin_post"

output = generate_output(test_transcript, platform)
print(f"Output:\n{output}")